{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file: data_file_1.csv\n",
      "Generated file: data_file_2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_csv_files(num_rows, num_files):\n",
    "    \"\"\"\n",
    "    Generate CSV files with specified number of rows and files.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): Number of rows per CSV file.\n",
    "        num_files (int): Total number of CSV files to create.\n",
    "    \"\"\"\n",
    "    # Define the columns\n",
    "    columns = [\n",
    "        \"exported\", \"productid\", \"product\", \"price\", \"depacher\", \"arrival\",\n",
    "        \"dateofcreation\", \"location\", \"age\", \"group\", \"usage\", \"product_age\"\n",
    "    ]\n",
    "\n",
    "    for file_counter in range(1, num_files + 1):\n",
    "        file_name = f\"data_file_{file_counter}.csv\"\n",
    "        rows = []\n",
    "\n",
    "        # Generate specified number of rows for the current file\n",
    "        for _ in range(num_rows):\n",
    "            row = {\n",
    "                \"exported\": random.choice([\"Yes\", \"No\"]),\n",
    "                \"productid\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),\n",
    "                \"product\": f\"Product-{random.randint(1, 500)}\",\n",
    "                \"price\": round(random.uniform(10.0, 1000.0), 2),\n",
    "                \"depacher\": f\"Dep-{random.randint(1, 50)}\",\n",
    "                \"arrival\": f\"Arr-{random.randint(1, 50)}\",\n",
    "                \"dateofcreation\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"location\": random.choice([\"Mumbai\", \"Delhi\", \"Bangalore\", \"Pune\"]),\n",
    "                \"age\": random.randint(18, 60),\n",
    "                \"group\": random.choice([\"A\", \"B\", \"C\", \"D\"]),\n",
    "                \"usage\": random.choice([\"Low\", \"Medium\", \"High\"]),\n",
    "                \"product_age\": random.randint(1, 10),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "        # Write data to a CSV file\n",
    "        with open(file_name, mode=\"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "\n",
    "        print(f\"Generated file: {file_name}\")\n",
    "\n",
    "        # Wait 10 seconds before generating the next file (if more files are to be created)\n",
    "        if file_counter < num_files:\n",
    "            time.sleep(10)\n",
    "\n",
    "# Example usage: Create 5 CSV files with 100 rows each\n",
    "generate_csv_files(num_rows=100, num_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PartialCredentialsError",
     "evalue": "Partial credentials found in env, missing: AWS_SECRET_ACCESS_KEY",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPartialCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(wait_time)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Example usage: Generate 2 CSV files with 100 rows each and upload them to S3\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mgenerate_csv_files_to_s3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myash-de\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     87\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mgenerate_csv_files_to_s3\u001b[1;34m(num_rows, num_files, bucket_name, s3_folder, wait_time)\u001b[0m\n\u001b[0;32m     27\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexported\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproductid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepacher\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrival\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdateofcreation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_age\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m ]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize S3 client\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m s3_client \u001b[38;5;241m=\u001b[39m \u001b[43mboto3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_counter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_files \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     36\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_file_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\boto3\\__init__.py:92\u001b[0m, in \u001b[0;36mclient\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclient\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    Create a low-level service client by name using the default session.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    See :py:meth:`boto3.session.Session.client`.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_default_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\boto3\\session.py:297\u001b[0m, in \u001b[0;36mSession.client\u001b[1;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclient\u001b[39m(\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    217\u001b[0m     service_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m     config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    227\u001b[0m ):\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    Create a low-level service client by name.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ssl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ssl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43maws_access_key_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_access_key_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43maws_secret_access_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_secret_access_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43maws_session_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_session_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\session.py:957\u001b[0m, in \u001b[0;36mSession.create_client\u001b[1;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PartialCredentialsError(\n\u001b[0;32m    951\u001b[0m         provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    952\u001b[0m         cred_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_cred_vars(\n\u001b[0;32m    953\u001b[0m             aws_access_key_id, aws_secret_access_key\n\u001b[0;32m    954\u001b[0m         ),\n\u001b[0;32m    955\u001b[0m     )\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 957\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    958\u001b[0m auth_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_auth_token()\n\u001b[0;32m    959\u001b[0m endpoint_resolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_internal_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendpoint_resolver\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\session.py:515\u001b[0m, in \u001b[0;36mSession.get_credentials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03mReturn the :class:`botocore.credential.Credential` object\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;124;03massociated with this session.  If the credentials have not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_components\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_component\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcredential_provider\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m--> 515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\credentials.py:2074\u001b[0m, in \u001b[0;36mCredentialResolver.load_credentials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m provider \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproviders:\n\u001b[0;32m   2073\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooking for credentials via: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, provider\u001b[38;5;241m.\u001b[39mMETHOD)\n\u001b[1;32m-> 2074\u001b[0m     creds \u001b[38;5;241m=\u001b[39m \u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m creds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2076\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m creds\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\credentials.py:1149\u001b[0m, in \u001b[0;36mEnvProvider.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound credentials in environment variables.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1148\u001b[0m fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_credentials_fetcher()\n\u001b[1;32m-> 1149\u001b[0m credentials \u001b[38;5;241m=\u001b[39m \u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequire_expiry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1151\u001b[0m expiry_time \u001b[38;5;241m=\u001b[39m credentials[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpiry_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expiry_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\YashMohite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\credentials.py:1189\u001b[0m, in \u001b[0;36mEnvProvider._create_credentials_fetcher.<locals>.fetch_credentials\u001b[1;34m(require_expiry)\u001b[0m\n\u001b[0;32m   1187\u001b[0m secret_key \u001b[38;5;241m=\u001b[39m environ\u001b[38;5;241m.\u001b[39mget(mapping[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecret_key\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m secret_key:\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PartialCredentialsError(\n\u001b[0;32m   1190\u001b[0m         provider\u001b[38;5;241m=\u001b[39mmethod, cred_var\u001b[38;5;241m=\u001b[39mmapping[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecret_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1191\u001b[0m     )\n\u001b[0;32m   1192\u001b[0m credentials[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecret_key\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m secret_key\n\u001b[0;32m   1194\u001b[0m credentials[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mPartialCredentialsError\u001b[0m: Partial credentials found in env, missing: AWS_SECRET_ACCESS_KEY"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "\n",
    "def generate_csv_files_to_s3(num_rows, num_files, bucket_name, s3_folder, wait_time=10):\n",
    "    \"\"\"\n",
    "    Generate CSV files and upload them to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): Number of rows per CSV file.\n",
    "        num_files (int): Total number of CSV files to create.\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "        s3_folder (str): Folder path within the S3 bucket where files will be stored.\n",
    "        wait_time (int): Time to wait (in seconds) between generating files. Default is 10 seconds.\n",
    "\n",
    "    Examples for wait_time:\n",
    "        - 30 minutes: wait_time = 30 * 60  (1800 seconds)\n",
    "        - 1 hour: wait_time = 60 * 60  (3600 seconds)\n",
    "        - 4 hours: wait_time = 4 * 60 * 60  (14400 seconds)\n",
    "        - 1 day: wait_time = 24 * 60 * 60  (86400 seconds)\n",
    "    \"\"\"\n",
    "    # Define the columns\n",
    "    columns = [\n",
    "        \"exported\", \"productid\", \"product\", \"price\", \"depacher\", \"arrival\",\n",
    "        \"dateofcreation\", \"location\", \"age\", \"group\", \"usage\", \"product_age\"\n",
    "    ]\n",
    "\n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    for file_counter in range(1, num_files + 1):\n",
    "        file_name = f\"data_file_{file_counter}.csv\"\n",
    "        rows = []\n",
    "\n",
    "        # Generate specified number of rows for the current file\n",
    "        for _ in range(num_rows):\n",
    "            row = {\n",
    "                \"exported\": random.choice([\"Yes\", \"No\"]),\n",
    "                \"productid\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),\n",
    "                \"product\": f\"Product-{random.randint(1, 500)}\",\n",
    "                \"price\": round(random.uniform(10.0, 1000.0), 2),\n",
    "                \"depacher\": f\"Dep-{random.randint(1, 50)}\",\n",
    "                \"arrival\": f\"Arr-{random.randint(1, 50)}\",\n",
    "                \"dateofcreation\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"location\": random.choice([\"Mumbai\", \"Delhi\", \"Bangalore\", \"Pune\"]),\n",
    "                \"age\": random.randint(18, 60),\n",
    "                \"group\": random.choice([\"A\", \"B\", \"C\", \"D\"]),\n",
    "                \"usage\": random.choice([\"Low\", \"Medium\", \"High\"]),\n",
    "                \"product_age\": random.randint(1, 10),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "        # Write data to a local CSV file\n",
    "        with open(file_name, mode=\"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "\n",
    "        print(f\"Generated file: {file_name}\")\n",
    "\n",
    "        # Upload the CSV file to the S3 bucket\n",
    "        s3_key = f\"{s3_folder}/{file_name}\"\n",
    "        try:\n",
    "            s3_client.upload_file(file_name, bucket_name, s3_key)\n",
    "            print(f\"Uploaded {file_name} to s3://{bucket_name}/{s3_key}\")\n",
    "        except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "            print(f\"Failed to upload {file_name} to S3: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading {file_name}: {e}\")\n",
    "\n",
    "        # Wait for the specified time before generating the next file (if more files are to be created)\n",
    "        if file_counter < num_files:\n",
    "            print(f\"Waiting {wait_time} seconds before generating the next file...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "# Example usage: Generate 2 CSV files with 100 rows each and upload them to S3\n",
    "generate_csv_files_to_s3(\n",
    "    num_rows=1000, \n",
    "    num_files=1, \n",
    "    bucket_name=\"yash-de\", \n",
    "    s3_folder=\"generated_data\",\n",
    "    wait_time=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class AWS_S3Manager:\n",
    "    def __init__(self, bucket_name, aws_access_key_id, aws_secret_access_key, region_name):\n",
    "        try:\n",
    "            if not bucket_name or not aws_access_key_id or not aws_secret_access_key or not region_name:\n",
    "                raise ValueError(\"All AWS credentials and bucket name must be provided.\")\n",
    "\n",
    "            self.bucket_name = bucket_name\n",
    "            self.s3_client = boto3.client(\n",
    "                service_name='s3',\n",
    "                region_name=region_name,\n",
    "                aws_access_key_id=aws_access_key_id,\n",
    "                aws_secret_access_key=aws_secret_access_key\n",
    "            )\n",
    "            # Check if bucket exists\n",
    "            self.s3_client.head_bucket(Bucket=self.bucket_name)\n",
    "            print(\"Connected to AWS S3 bucket\")\n",
    "        except self.s3_client.exceptions.NoSuchBucket:\n",
    "            raise Exception(f\"The bucket '{bucket_name}' does not exist.\", sys)\n",
    "        except Exception as e:\n",
    "            raise Exception(e, sys)\n",
    "\n",
    "    def upload_file(self, local_filename, s3_key):\n",
    "        \"\"\"\n",
    "        Upload file to S3 bucket.\n",
    "        Args:\n",
    "            local_filename (str): Local file path.\n",
    "            s3_key (str): S3 object key.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(local_filename):\n",
    "                raise FileNotFoundError(f\"The file '{local_filename}' does not exist.\")\n",
    "            if not s3_key:\n",
    "                raise ValueError(\"S3 key cannot be empty.\")\n",
    "\n",
    "            self.s3_client.upload_file(local_filename, self.bucket_name, s3_key)\n",
    "            print(f\"File '{local_filename}' uploaded to S3 bucket as '{s3_key}'.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(e, sys)\n",
    "\n",
    "    def upload_dataframe_to_s3(self, df, s3_key):\n",
    "        \"\"\"\n",
    "        Uploads a Pandas DataFrame to the S3 bucket with the specified key.\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to upload.\n",
    "            s3_key (str): The S3 key (object key) under which to store the DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                raise ValueError(\"The provided object is not a valid Pandas DataFrame.\")\n",
    "            if df.empty:\n",
    "                raise ValueError(\"The DataFrame is empty and cannot be uploaded.\")\n",
    "            if not s3_key:\n",
    "                raise ValueError(\"S3 key cannot be empty.\")\n",
    "\n",
    "            # Convert DataFrame to CSV format\n",
    "            csv_buffer = df.to_csv(index=False).encode('utf-8')\n",
    "\n",
    "            # Upload the CSV data to S3\n",
    "            self.s3_client.put_object(Bucket=self.bucket_name, Key=s3_key, Body=csv_buffer)\n",
    "            print(f\"DataFrame uploaded to S3 bucket as '{s3_key}'.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(e, sys)\n",
    "\n",
    "    def download_file(self, s3_key, local_filename, target_directory=\"data\"):\n",
    "        \"\"\"\n",
    "        Download file from S3 bucket.\n",
    "        Args:\n",
    "            s3_key (str): S3 object key.\n",
    "            local_filename (str): Local file name to save as.\n",
    "            target_directory (str): Directory to save the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not s3_key:\n",
    "                raise ValueError(\"S3 key cannot be empty.\")\n",
    "            if not local_filename:\n",
    "                raise ValueError(\"Local filename cannot be empty.\")\n",
    "            \n",
    "            if not os.path.exists(target_directory):\n",
    "                os.makedirs(target_directory)  # Create the target directory if it doesn't exist\n",
    "\n",
    "            # Construct the full local path\n",
    "            local_path = os.path.join(target_directory, local_filename)\n",
    "\n",
    "            # Check if the object exists in S3\n",
    "            self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "\n",
    "            # Download the file\n",
    "            self.s3_client.download_file(self.bucket_name, s3_key, local_path)\n",
    "            print(f\"File '{s3_key}' downloaded from S3 bucket and saved as '{local_path}'.\")\n",
    "        except self.s3_client.exceptions.NoSuchKey:\n",
    "            raise Exception(f\"The key '{s3_key}' does not exist in the bucket.\", sys)\n",
    "        except Exception as e:\n",
    "            print(\"Error\")\n",
    "\n",
    "    def read_csv_from_s3(self, s3_key):\n",
    "        \"\"\"\n",
    "        Reads CSV file from S3 bucket.\n",
    "        Args:\n",
    "            s3_key (str): S3 object key.\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not s3_key:\n",
    "                raise ValueError(\"S3 key cannot be empty.\")\n",
    "            \n",
    "            # Check if the object exists in S3\n",
    "            self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "\n",
    "            obj = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "            df = pd.read_csv(obj['Body'])\n",
    "            return df\n",
    "        except self.s3_client.exceptions.NoSuchKey:\n",
    "            raise Exception(f\"The key '{s3_key}' does not exist in the bucket.\", sys)\n",
    "        except Exception as e:\n",
    "            print(\"Error\")\n",
    "\n",
    "    def move_data(self, source_folder, destination_folder):\n",
    "        \"\"\"\n",
    "        Moves data in S3 bucket from source_folder to destination_folder.\n",
    "        Args:\n",
    "            source_folder (str): Source folder path in S3.\n",
    "            destination_folder (str): Destination folder path in S3.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not source_folder or not destination_folder:\n",
    "                raise ValueError(\"Source and destination folders cannot be empty.\")\n",
    "\n",
    "            # List objects in the source folder\n",
    "            response = self.s3_client.list_objects_v2(Bucket=self.bucket_name, Prefix=source_folder)\n",
    "            if 'Contents' in response:\n",
    "                for obj in response['Contents']:\n",
    "                    if not obj['Key'].endswith('/'):  # Ignore folders\n",
    "                        # Construct the new key destination path\n",
    "                        new_key = obj['Key'].replace(source_folder, destination_folder, 1)\n",
    "                        # Copy the object to the new location\n",
    "                        self.s3_client.copy_object(\n",
    "                            Bucket=self.bucket_name,\n",
    "                            CopySource={'Bucket': self.bucket_name, 'Key': obj['Key']},\n",
    "                            Key=new_key\n",
    "                        )\n",
    "                        # Delete the original object\n",
    "                        self.s3_client.delete_object(Bucket=self.bucket_name, Key=obj['Key'])\n",
    "                        print(f\"Moved {obj['Key']} to {new_key}\")\n",
    "            else:\n",
    "                print(f\"No objects found in {source_folder}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while moving data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to AWS S3 bucket\n"
     ]
    }
   ],
   "source": [
    "aws_manager = AWS_S3Manager(\n",
    "    bucket_name=os.getenv(\"S3_BUCKET_NAME\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY_ID\"),\n",
    "    region_name=os.getenv(\"AWS_REGION_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aws_manager.read_csv_from_s3(\n",
    "    s3_key=\"generated_data/data_file_1.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exported</th>\n",
       "      <th>productid</th>\n",
       "      <th>product</th>\n",
       "      <th>price</th>\n",
       "      <th>depacher</th>\n",
       "      <th>arrival</th>\n",
       "      <th>dateofcreation</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>usage</th>\n",
       "      <th>product_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>HFMMW7ZDAD</td>\n",
       "      <td>Product-83</td>\n",
       "      <td>824.57</td>\n",
       "      <td>Dep-46</td>\n",
       "      <td>Arr-19</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>38</td>\n",
       "      <td>B</td>\n",
       "      <td>Low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>CE78DVX34S</td>\n",
       "      <td>Product-224</td>\n",
       "      <td>832.19</td>\n",
       "      <td>Dep-13</td>\n",
       "      <td>Arr-32</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Pune</td>\n",
       "      <td>40</td>\n",
       "      <td>D</td>\n",
       "      <td>High</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>L67C3E0V55</td>\n",
       "      <td>Product-465</td>\n",
       "      <td>604.03</td>\n",
       "      <td>Dep-42</td>\n",
       "      <td>Arr-13</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>59</td>\n",
       "      <td>A</td>\n",
       "      <td>Low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>YQAXEY1M9X</td>\n",
       "      <td>Product-357</td>\n",
       "      <td>726.10</td>\n",
       "      <td>Dep-20</td>\n",
       "      <td>Arr-13</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>20</td>\n",
       "      <td>B</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>4WZY3XWNU8</td>\n",
       "      <td>Product-237</td>\n",
       "      <td>271.35</td>\n",
       "      <td>Dep-43</td>\n",
       "      <td>Arr-41</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>48</td>\n",
       "      <td>C</td>\n",
       "      <td>Low</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exported   productid      product   price depacher arrival  \\\n",
       "0       No  HFMMW7ZDAD   Product-83  824.57   Dep-46  Arr-19   \n",
       "1       No  CE78DVX34S  Product-224  832.19   Dep-13  Arr-32   \n",
       "2       No  L67C3E0V55  Product-465  604.03   Dep-42  Arr-13   \n",
       "3      Yes  YQAXEY1M9X  Product-357  726.10   Dep-20  Arr-13   \n",
       "4      Yes  4WZY3XWNU8  Product-237  271.35   Dep-43  Arr-41   \n",
       "\n",
       "        dateofcreation location  age group usage  product_age  \n",
       "0  2025-01-23 17:41:06    Delhi   38     B   Low            4  \n",
       "1  2025-01-23 17:41:06     Pune   40     D  High            3  \n",
       "2  2025-01-23 17:41:06   Mumbai   59     A   Low            4  \n",
       "3  2025-01-23 17:41:06    Delhi   20     B  High            1  \n",
       "4  2025-01-23 17:41:06    Delhi   48     C   Low            7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "def process_data(dataframe):\n",
    "    def transform_data(df_to_transform):\n",
    "        current_time = datetime.now()\n",
    "        df_to_transform['transformation_date'] = current_time.strftime('%Y-%m-%d')\n",
    "        df_to_transform['transformation_time'] = current_time.strftime('%H:%M:%S')\n",
    "        \n",
    "        # Add transformations\n",
    "        df_to_transform['price_category'] = df_to_transform['price'].apply(\n",
    "            lambda x: 'Low' if x < 200 else 'Medium' if x < 600 else 'High'\n",
    "        )\n",
    "        df_to_transform['transit_time'] = df_to_transform.apply(\n",
    "            lambda row: abs(int(row['depacher'].split('-')[-1]) - int(row['arrival'].split('-')[-1])), axis=1\n",
    "        )\n",
    "        df_to_transform['normalized_product_age'] = (\n",
    "            (df_to_transform['product_age'] - df_to_transform['product_age'].min()) /\n",
    "            (df_to_transform['product_age'].max() - df_to_transform['product_age'].min())\n",
    "        )\n",
    "        df_to_transform['region'] = df_to_transform['location'].apply(\n",
    "            lambda loc: 'North' if loc in ['Delhi', 'Mumbai'] else 'South'\n",
    "        )\n",
    "        df_to_transform['export_flag'] = 1\n",
    "        df_to_transform['high_value_flag'] = df_to_transform.apply(\n",
    "            lambda row: 1 if row['price'] > 600 and row['usage'] == 'High' else 0, axis=1\n",
    "        )\n",
    "        df_to_transform['age_group'] = df_to_transform['age'].apply(\n",
    "            lambda age: 'Youth' if age < 30 else 'Adult' if age < 50 else 'Senior'\n",
    "        )\n",
    "        return df_to_transform\n",
    "\n",
    "    # Split the data into two DataFrames\n",
    "    exported_yes_df = dataframe[dataframe['exported'] == 'Yes'].copy()\n",
    "    exported_no_df = dataframe[dataframe['exported'] == 'No'].copy()\n",
    "\n",
    "    # Transform the `exported == Yes` DataFrame\n",
    "    transformed_exported_yes_df = transform_data(exported_yes_df)\n",
    "\n",
    "    # Simulate processing `exported == No` after 5 minutes\n",
    "    print(\"Waiting 5 minutes to process 'exported == No' rows...\")\n",
    "    time.sleep(60)  # Simulates 5 minutes delay\n",
    "\n",
    "    # Update `exported == No` DataFrame and process\n",
    "    current_time = datetime.now()\n",
    "    exported_no_df['exported'] = 'Yes'\n",
    "    exported_no_df['dateofcreation'] = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Apply transformations to `exported == No`\n",
    "    transformed_exported_no_df = transform_data(exported_no_df)\n",
    "\n",
    "    # Concatenate the transformed DataFrames\n",
    "    final_df = pd.concat([transformed_exported_yes_df, transformed_exported_no_df])\n",
    "    \n",
    "    finaldf = final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 5 minutes to process 'exported == No' rows...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exported</th>\n",
       "      <th>productid</th>\n",
       "      <th>product</th>\n",
       "      <th>price</th>\n",
       "      <th>depacher</th>\n",
       "      <th>arrival</th>\n",
       "      <th>dateofcreation</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>...</th>\n",
       "      <th>product_age</th>\n",
       "      <th>transformation_date</th>\n",
       "      <th>transformation_time</th>\n",
       "      <th>price_category</th>\n",
       "      <th>transit_time</th>\n",
       "      <th>normalized_product_age</th>\n",
       "      <th>region</th>\n",
       "      <th>export_flag</th>\n",
       "      <th>high_value_flag</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>YQAXEY1M9X</td>\n",
       "      <td>Product-357</td>\n",
       "      <td>726.10</td>\n",
       "      <td>Dep-20</td>\n",
       "      <td>Arr-13</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>20</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>21:40:52</td>\n",
       "      <td>High</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>North</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>4WZY3XWNU8</td>\n",
       "      <td>Product-237</td>\n",
       "      <td>271.35</td>\n",
       "      <td>Dep-43</td>\n",
       "      <td>Arr-41</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>48</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>21:40:52</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>North</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yes</td>\n",
       "      <td>213HYDA0XS</td>\n",
       "      <td>Product-493</td>\n",
       "      <td>150.10</td>\n",
       "      <td>Dep-17</td>\n",
       "      <td>Arr-5</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>55</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>21:40:52</td>\n",
       "      <td>Low</td>\n",
       "      <td>12</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>North</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes</td>\n",
       "      <td>2XLMOBBF6Y</td>\n",
       "      <td>Product-48</td>\n",
       "      <td>954.71</td>\n",
       "      <td>Dep-12</td>\n",
       "      <td>Arr-15</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>31</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>21:40:52</td>\n",
       "      <td>High</td>\n",
       "      <td>3</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>South</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yes</td>\n",
       "      <td>9N0OOXJ4IM</td>\n",
       "      <td>Product-110</td>\n",
       "      <td>334.03</td>\n",
       "      <td>Dep-30</td>\n",
       "      <td>Arr-37</td>\n",
       "      <td>2025-01-23 17:41:06</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>28</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>21:40:52</td>\n",
       "      <td>Medium</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>North</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  exported   productid      product   price depacher arrival  \\\n",
       "3      Yes  YQAXEY1M9X  Product-357  726.10   Dep-20  Arr-13   \n",
       "4      Yes  4WZY3XWNU8  Product-237  271.35   Dep-43  Arr-41   \n",
       "5      Yes  213HYDA0XS  Product-493  150.10   Dep-17   Arr-5   \n",
       "6      Yes  2XLMOBBF6Y   Product-48  954.71   Dep-12  Arr-15   \n",
       "8      Yes  9N0OOXJ4IM  Product-110  334.03   Dep-30  Arr-37   \n",
       "\n",
       "        dateofcreation   location  age group  ... product_age  \\\n",
       "3  2025-01-23 17:41:06      Delhi   20     B  ...           1   \n",
       "4  2025-01-23 17:41:06      Delhi   48     C  ...           7   \n",
       "5  2025-01-23 17:41:06     Mumbai   55     C  ...           6   \n",
       "6  2025-01-23 17:41:06  Bangalore   31     C  ...           3   \n",
       "8  2025-01-23 17:41:06     Mumbai   28     D  ...           7   \n",
       "\n",
       "   transformation_date transformation_time price_category transit_time  \\\n",
       "3           2025-01-24            21:40:52           High            7   \n",
       "4           2025-01-24            21:40:52         Medium            2   \n",
       "5           2025-01-24            21:40:52            Low           12   \n",
       "6           2025-01-24            21:40:52           High            3   \n",
       "8           2025-01-24            21:40:52         Medium            7   \n",
       "\n",
       "   normalized_product_age  region export_flag  high_value_flag  age_group  \n",
       "3                0.000000   North           1                1      Youth  \n",
       "4                0.666667   North           1                0      Adult  \n",
       "5                0.555556   North           1                0     Senior  \n",
       "6                0.222222   South           1                0      Adult  \n",
       "8                0.666667   North           1                0      Youth  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = process_data(df)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataframe_to_s3(\n",
    "    df, \n",
    "    bucket_name=\"your-s3-bucket\", \n",
    "    aws_access_key=None,\n",
    "    aws_secret_key=None,\n",
    "    folder=\"transform_json_de_products_data\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Save the provided DataFrame directly as JSON to S3.\n",
    "    The file name is dynamically determined by the existing files in the S3 folder.\n",
    "    \"\"\"\n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=aws_access_key, \n",
    "        aws_secret_access_key=aws_secret_key,\n",
    "    )\n",
    "    \n",
    "    # Get the existing files in the specified S3 folder\n",
    "    existing_files = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder).get('Contents', [])\n",
    "    \n",
    "    # If the folder is empty, existing_files will be empty, handle this case\n",
    "    if existing_files:\n",
    "        # Extract file names to count how many files already exist\n",
    "        existing_files = [f['Key'] for f in existing_files]\n",
    "    else:\n",
    "        existing_files = []\n",
    "\n",
    "    # Determine the next file number (e.g., data_file1.json, data_file2.json, ...)\n",
    "    file_number = len(existing_files) + 1\n",
    "    json_filename = f\"{folder}/data_file{file_number}.json\"\n",
    "\n",
    "    # Convert the DataFrame to JSON string (in records format, one line per record)\n",
    "    json_data = df.to_json(orient=\"records\", lines=True)\n",
    "\n",
    "    # Upload the JSON string directly to S3\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=json_filename, Body=json_data)\n",
    "    print(f\"Data saved to S3 bucket '{bucket_name}' at location '{json_filename}'\")\n",
    "\n",
    "    return json_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to S3 bucket 'yash-de' at location 'transform_json_de_products_data/data_file2.json'\n"
     ]
    }
   ],
   "source": [
    "final_df = final_df  \n",
    "bucket_name = 'yash-de'  # Replace with your actual S3 bucket name\n",
    "json_file = save_dataframe_to_s3(\n",
    "    final_df, \n",
    "    bucket_name=bucket_name,\n",
    "    aws_access_key=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_key=os.getenv(\"AWS_SECRET_ACCESS_KEY_ID\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import textwrap\n",
    "import openpyxl\n",
    "\n",
    "class PostgresConnector:\n",
    "    def __init__(self, connection) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the PostgresConnector with a connection and sets the cursor to use RealDictCursor.\n",
    "\n",
    "        Args:\n",
    "            connection (dict): A dictionary containing the connection parameters for psycopg2.\n",
    "        \"\"\"\n",
    "        self.conn = psycopg2.connect(**connection)\n",
    "        self.cursor = self.conn.cursor(cursor_factory=RealDictCursor)  \n",
    "        self._set_timezone_utc()\n",
    "\n",
    "    def _set_timezone_utc(self):\n",
    "        \"\"\"\n",
    "        Set the timezone for the database connection to UTC.\n",
    "        \"\"\"\n",
    "        if self.conn:\n",
    "            self.cursor.execute(\"SET TIME ZONE 'UTC';\")\n",
    "            \n",
    "    def execute_cr_table_query(self, query):\n",
    "        \"\"\"\n",
    "        Execute a single query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.cursor:\n",
    "                self.cursor.execute(query)\n",
    "                self.conn.commit()\n",
    "                print(\"Query executed successfully.\")\n",
    "            else:\n",
    "                print(\"No active database connection.\")\n",
    "        except psycopg2.Error as e:\n",
    "            print(\"Error executing query:\", e)\n",
    "\n",
    "    def execute_query(self, query, params=None, print_query: bool = True):\n",
    "        \"\"\"\n",
    "        Execute a SQL query and return the results as a list of dictionaries where each dictionary\n",
    "        represents a row with column names as keys.\n",
    "\n",
    "        Args:\n",
    "            query (str): The SQL query string to be executed.\n",
    "            params (tuple, optional): parameters to pass with the query.\n",
    "            print_query (bool, optional): For printing query on the terminal.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, where each dictionary represents a row with column names as keys,\n",
    "                  if the query is a SELECT statement.\n",
    "            int: The number of affected rows for non-SELECT queries.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if print_query:\n",
    "                print(\"\\n\", textwrap.dedent(query))\n",
    "            self.cursor.execute(query, params)\n",
    "            if query.strip().upper().startswith((\"SELECT\", \"WITH\")):\n",
    "                result = self.cursor.fetchall()\n",
    "                result_list = [dict(row) for row in result]\n",
    "                return result_list\n",
    "            else:\n",
    "                affected_rows = self.cursor.rowcount\n",
    "                self.conn.commit()\n",
    "                return affected_rows\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(\"\\n\", textwrap.dedent(query))\n",
    "            raise e\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the database connection and cursor.\n",
    "        \"\"\"\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "\n",
    "    def copy_from_csv(self, csv_buffer, schema_name, table_name) -> None:\n",
    "        \"\"\"\n",
    "        Copy data from a CSV buffer to a database table.\n",
    "\n",
    "        Args:\n",
    "            csv_buffer (Any): A file-like object containing CSV data.\n",
    "            schema_name (str): The name of the schema containing the table.\n",
    "            table_name (str): The name of the table to copy data into.\n",
    "\n",
    "        Raises:\n",
    "            psycopg2.Error: If there's an error during the copy operation.\n",
    "        \"\"\"\n",
    "        copy_query = (\n",
    "            f\"COPY {schema_name}.{table_name} FROM STDIN WITH CSV HEADER DELIMITER ',' NULL AS ''\"\n",
    "        )\n",
    "        try:\n",
    "            self.cursor.copy_expert(copy_query, csv_buffer)\n",
    "            self.conn.commit()\n",
    "        except psycopg2.Error as e:\n",
    "            self.conn.rollback()\n",
    "            raise e\n",
    "\n",
    "    def insert_from_excel(self, schema_name, table_name, file_path):\n",
    "        \"\"\"\n",
    "        Create a table and insert data from an Excel file into the specified schema and table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.cursor:\n",
    "                print(\"No active database connection.\")\n",
    "                return\n",
    "\n",
    "            # Load Excel file\n",
    "            workbook = openpyxl.load_workbook(file_path)\n",
    "            sheet = workbook.active\n",
    "\n",
    "            # Extract headers and create table columns\n",
    "            columns = [cell.value for cell in sheet[1]]\n",
    "            column_definitions = ', '.join([f'\"{col}\" TEXT' for col in columns])\n",
    "\n",
    "            # Create schema if not exists\n",
    "            self.execute_query(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n",
    "\n",
    "            # Create table if not exists\n",
    "            create_table_query = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n",
    "                    {column_definitions}\n",
    "                );\n",
    "            \"\"\"\n",
    "            self.execute_query(create_table_query)\n",
    "\n",
    "            # Prepare insert query\n",
    "            placeholders = ', '.join(['%s' for _ in columns])\n",
    "            quoted_columns = ', '.join([f'\"{col}\"' for col in columns])\n",
    "            insert_query = f\"INSERT INTO {schema_name}.{table_name} ({quoted_columns}) VALUES ({placeholders});\"\n",
    "\n",
    "            # Insert data from Excel into the table\n",
    "            for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "                self.cursor.execute(insert_query, row)\n",
    "\n",
    "            # Commit all changes\n",
    "            self.conn.commit()\n",
    "            print(\"Data inserted successfully from Excel.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error during Excel data insertion:\", e)\n",
    "\n",
    "    def copy_from_dataframe(self, df: pd.DataFrame, schema_name: str, table_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Copy a pandas DataFrame directly into a PostgreSQL table.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The pandas DataFrame to copy.\n",
    "            schema_name (str): The schema name of the target table.\n",
    "            table_name (str): The target table name.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                print(\"DataFrame is empty. No data to copy.\")\n",
    "                return\n",
    "\n",
    "            # Convert DataFrame to a CSV buffer\n",
    "            csv_buffer = StringIO()\n",
    "            df.to_csv(csv_buffer, index=False, header=True)\n",
    "            csv_buffer.seek(0)\n",
    "\n",
    "            # Use the copy_from_csv method to copy data into the database\n",
    "            self.copy_from_csv(csv_buffer, schema_name, table_name)\n",
    "            print(\"DataFrame copied successfully to the table.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error during DataFrame copy:\", e)\n",
    "            self.conn.rollback()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': '**********', 'user': 'yashm', 'password': '************', 'host': '**********', 'port': '5432'}\n"
     ]
    }
   ],
   "source": [
    "CRED = {\n",
    "    'database': '**********',\n",
    "    'user': 'yashm',\n",
    "    'password': '************',\n",
    "    'host': '**********',\n",
    "    'port': '5432'\n",
    "}\n",
    "print(CRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_conn = PostgresConnector(CRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CREATE SCHEMA IF NOT EXISTS ym_de_products;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_conn.execute_query(\"CREATE SCHEMA IF NOT EXISTS ym_de_products;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully.\n"
     ]
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ym_de_products.product_table (\n",
    "    exported TEXT,\n",
    "    productid TEXT,\n",
    "    product TEXT,\n",
    "    price FLOAT8,\n",
    "    depacher TEXT,\n",
    "    arrival TEXT,\n",
    "    dateofcreation DATE,\n",
    "    location TEXT,\n",
    "    age INT,\n",
    "    \"group\" TEXT,\n",
    "    usage TEXT,\n",
    "    product_age INT,\n",
    "    transformation_date DATE,\n",
    "    transformation_time TIME,\n",
    "    price_category TEXT,\n",
    "    transit_time INT,\n",
    "    normalized_product_age FLOAT8,\n",
    "    region TEXT,\n",
    "    export_flag BOOLEAN,\n",
    "    high_value_flag BOOLEAN,\n",
    "    age_group TEXT\n",
    ");\n",
    "\"\"\"\n",
    "pg_conn.execute_cr_table_query(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame copied successfully to the table.\n"
     ]
    }
   ],
   "source": [
    "pg_conn.copy_from_dataframe(final_df, 'ym_de_products', 'product_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_manager.move_data(\n",
    "    source_folder=\"generated_data/data_file_1.csv\", \n",
    "    destination_folder=\"pgenerated_data_archive_files/data_file_1.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
